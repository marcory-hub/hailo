{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzPOf8/juxSNtHMM0brqIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcory-hub/hailo/blob/main/DFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Prepair you dataset\n",
        "\n",
        "\n",
        "1. Dataset Structure:\n",
        "  \n",
        "  Make a annotated dataset with this folder and naming structure. If you do not have your own dataset, you can download the [hornet3000+ dataset](https://www.kaggle.com/datasets/marcoryvandijk/vespa-velutina-v-crabro-vespulina-vulgaris). If you want to annotate your own dataset take a look at CVAT or Roboflow to annotate images and save it in YOLO format.\n",
        "\n",
        "```\n",
        "data/\n",
        "  train/\n",
        "    images/\n",
        "      image_1.jpg  # Image file\n",
        "      image_2.png  # Image file\n",
        "      etc...\n",
        "    labels/\n",
        "      image_1.txt  # Label file\n",
        "      image_2.txt  # Label file\n",
        "      etc...\n",
        "  val/\n",
        "    images/\n",
        "      image_3.jpg  # Image for validation\n",
        "      image_4.png  # Image for validation\n",
        "      ...\n",
        "    labels/\n",
        "      image_3.txt  # Label file for validation\n",
        "      image_4.txt  # Label file for validation\n",
        "      ...\n",
        "```\n",
        "\n",
        "2. data.yaml Configuration (Optional)\n",
        "\n",
        "  Add `data.yaml` (configuration file used by the training script to locate the data) to folder datasets. This contains:\n",
        "  - absolute path to train images (train)\n",
        "  - absolute path validation (val)\n",
        "  - the number of classes (nc)\n",
        "  - and the class names (names)\n",
        "\n",
        "  For example a dataset with 3 types of insects would look like this:\n",
        "```\n",
        "train: /content/data/train # path to train images\n",
        "val: /content/data/val # path to val images\n",
        "nc: 3\n",
        "names: ['Vespa_velutina', 'Vespa_crabro', 'Vespula_vulgaris']\n",
        "```\n",
        "Make sure to adjust the `nc` (number of classes) and `names` accordingly.\n",
        "\n",
        "3. Zip Your Dataset (Optional):\n",
        "\n",
        "  Only if you use own dataset: zip the data folder to a file names `dataset.zip` (or a custom name). On mac use `ditto -c -k --norsrc --keepParent images dataset.zip` to exclude finderfiles from the zipped file.\n",
        "\n",
        "4. Copy the zipped dataset:\n",
        "  \n",
        "  Copy the zipped file with dataset and yaml to your google drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "rqDdKd2X_Mns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Unzip dataset.zip and rename the folder on google drive\n",
        "\n",
        "Adjust the names of the `dataset_path` and `dataset_filename` in the boxes on the right."
      ],
      "metadata": {
        "id": "cuSaGAA8DyDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Unzip dataset.zip and rename the folder on google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SUPnZCbI96Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define Paths with Parameters\n",
        "dataset_path = \"/content/gdrive/MyDrive/vespA/100_1_dataset.zip\"  # @param {type:\"string\"}\n",
        "dataset_filename = \"100_1_sampleSize\"  # @param {type:\"string\"}\n",
        "\n",
        "# Unzip the Dataset (using the defined path)\n",
        "!unzip {dataset_path} -d '/content/'\n",
        "\n",
        "# Rename the Extracted Folder\n",
        "old_path = f'/content/{dataset_filename}'\n",
        "new_path = '/content/dataset'\n",
        "os.rename(old_path, new_path)"
      ],
      "metadata": {
        "id": "05bLQk36i-94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Check is path is correct\n",
        "\n",
        "output should be\n",
        "- train valid\n",
        "- images labels\n",
        "- /content/gdrive/MyDrive/vespA/data.yaml\n"
      ],
      "metadata": {
        "id": "WGlnv5GXtaz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional chech datapaths\n",
        "\n",
        "!ls '/content/dataset/'\n",
        "!ls '/content/dataset/train/'\n",
        "!ls '/content/dataset/valid/'\n",
        "!ls '/content/gdrive/MyDrive/vespA/data.yaml'"
      ],
      "metadata": {
        "id": "bbYyE7zctW_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train YOLO11 model"
      ],
      "metadata": {
        "id": "BZJitLHv6AGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install ultralytics"
      ],
      "metadata": {
        "id": "6pb6wMhVHd0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the python package\n",
        "!pip install ultralytics\n",
        "\n",
        "#Verifying the installation\n",
        "!pip show ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "0PqKpDtbHgKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Retrain model"
      ],
      "metadata": {
        "id": "rZGcPMJIHlJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model_name = \"yolo11s\" #@param {type:\"string\"}\n",
        "dataset_path = \"/content/gdrive/MyDrive/vespA/data.yaml\" #@param {type:\"string\"}\n",
        "\n",
        "# Get pre-trained model\n",
        "model = YOLO(f\"{model_name}.pt\")\n",
        "\n",
        "# Train/fine-tune model\n",
        "model.train(project=\"yolo11vespA\",  # wandb project name\n",
        "            name=\"train1\",          # run name wandb\n",
        "            data=dataset_path,\n",
        "            epochs=3,\n",
        "            imgsz=640,\n",
        "            fraction=0.1)"
      ],
      "metadata": {
        "id": "x6WpYNS4HkxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Validate model"
      ],
      "metadata": {
        "id": "tuCAatGuHqKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=val \\\n",
        "model=\"/path/to/YoloRetrained/weights/best.pt\" \\\n",
        "    data=\"/path/to/dataset.yaml\""
      ],
      "metadata": {
        "id": "MTZsycVfHupF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Convert .pt file to ONNX via CLI"
      ],
      "metadata": {
        "id": "d33KrIaIHy2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=path/to/best.pt format=onnx  # export custom trained model"
      ],
      "metadata": {
        "id": "WnxbaG2NH3T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR\n",
        "\n",
        "convert .pt file to ONNX via python code"
      ],
      "metadata": {
        "id": "27rMhIynIKOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load our model into our environment\n",
        "checkpoint = torch.load('/path/to/best.pt')\n",
        "model = checkpoint['model']\n",
        "\n",
        "model = model.float()\n",
        "model.eval()\n",
        "\n",
        "# Dummy input in FP32\n",
        "dummy_input = torch.randn(16, 3, 640, 640, dtype=torch.float)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"modified_run_3.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,  # Adjust opset version if needed\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output']\n",
        ")\n",
        "print(\"ONNX model exported successfully!\")"
      ],
      "metadata": {
        "id": "JcCiYhosIKDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Verify the model"
      ],
      "metadata": {
        "id": "nlgmtDFQIgpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(\"modified_run_3.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"ONNX model is valid!\")\n",
        "\n",
        "# Test the ONNX model with ONNX Runtime\n",
        "dummy_input = torch.randn(16, 3, 640, 640).numpy()\n",
        "ort_session = ort.InferenceSession(\"modified_run_3.onnx\")\n",
        "outputs = ort_session.run(None, {\"input\": dummy_input})\n",
        "print(outputs[0])"
      ],
      "metadata": {
        "id": "j8eatV0iIJey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hailo dataflow compiler"
      ],
      "metadata": {
        "id": "bqAGJvQUI1UQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Make virtual environment in Colab"
      ],
      "metadata": {
        "id": "jtouN6GbJjJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-dev python3-distutils python3-tk libfuse2 graphviz libgraphviz-dev\n",
        "\n",
        "# Will need a venv to install the DFC in\n",
        "!pip install --upgrade pip virtualenv\n",
        "!virtualenv my_env"
      ],
      "metadata": {
        "id": "XNtqRTnpJm2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download hailo dataflow compiler (python 3.10) from https://hailo.ai/developer-zone/software-downloads/ (you need to make an account)\n",
        "\n",
        "Then copy the .whl to google drive\n"
      ],
      "metadata": {
        "id": "LwsOA_V3I4zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the WHL file for Hailo DFC\n",
        "!my_env/bin/pip install /content/hailo_dataflow_compiler-3.29.0-py3-none-linux_x86_64.whl\n",
        "\n",
        "# Making sure it's installed properly\n",
        "!my_env/bin/hailo --version"
      ],
      "metadata": {
        "id": "p71hAN7HJf7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying the six end node names via www.netron.app\n",
        "\n",
        "To identify the Yolo’s end nodes, they are the nodes right before the post-processing operations at the very bottom of the model. Their are 2 end nodes per map\n",
        "\n",
        "In yolov11 model\n",
        "```\n",
        "\"/model.23/cv2.2/cv2.2.2/Conv\",\n",
        "\"/model.23/cv3.2/cv3.2.2/Conv\",\n",
        "\"/model.23/cv2.1/cv2.1.2/Conv\",\n",
        "\"/model.23/cv3.1/cv3.1.2/Conv\",\n",
        "\"/model.23/cv2.0/cv2.0.2/Conv\",\n",
        "\"/model.23/cv3.0/cv3.0.2/Conv\",\n",
        "```\n"
      ],
      "metadata": {
        "id": "euWouzZiKcEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Parsing to ONNX file"
      ],
      "metadata": {
        "id": "d9iWYW7QLWh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"translate_model.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Define the ONNX model path and configuration\n",
        "onnx_path = \"/content/modified_run_3.onnx\"  # Replace with your ONNX model path\n",
        "onnx_model_name = \"modified_run_3_renamed\"\n",
        "chosen_hw_arch = \"hailo8l\"  # Specify the target hardware architecture\n",
        "\n",
        "# Initialize the ClientRunner\n",
        "runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
        "\n",
        "# Use the recommended end node names for translation\n",
        "end_node_names = [\n",
        "  \"/model.23/cv2.2/cv2.2.2/Conv\",\n",
        "  \"/model.23/cv3.2/cv3.2.2/Conv\",\n",
        "  \"/model.23/cv2.1/cv2.1.2/Conv\",\n",
        "  \"/model.23/cv3.1/cv3.1.2/Conv\",\n",
        "  \"/model.23/cv2.0/cv2.0.2/Conv\",\n",
        "  \"/model.23/cv3.0/cv3.0.2/Conv\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Translate the ONNX model to Hailo's format\n",
        "    hn, npz = runner.translate_onnx_model(\n",
        "        onnx_path,\n",
        "        onnx_model_name,\n",
        "        end_node_names=end_node_names,\n",
        "        net_input_shapes={\"images\": [1, 3, 320, 320]},  # Adjust input shapes if needed\n",
        "    )\n",
        "    print(\"Model translation successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during model translation: {e}\")\n",
        "    raise\n",
        "\n",
        "# Save the Hailo model HAR file\n",
        "hailo_model_har_name = f\"{onnx_model_name}_hailo_model.har\"\n",
        "try:\n",
        "    runner.save_har(hailo_model_har_name)\n",
        "    print(f\"HAR file saved as: {hailo_model_har_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving HAR file: {e}\")\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "8_VrxC1pIsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model in CLI\n",
        "!my_env/bin/python translate_model.py"
      ],
      "metadata": {
        "id": "gidGCc-wLf1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Optimize model"
      ],
      "metadata": {
        "id": "CR5d4kdrLlJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Load the HAR file\n",
        "har_path = \"modified_run_3_renamed_hailo_model.har\"\n",
        "\n",
        "runner = ClientRunner(har=har_path)\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "try:\n",
        "    # Access the HailoNet as an OrderedDict\n",
        "    hn_dict = runner.get_hn()  # Or use runner._hn if get_hn() is unavailable\n",
        "    print(\"Inspecting layers from HailoNet (OrderedDict):\")\n",
        "\n",
        "    # Pretty-print each layer\n",
        "    for key, value in hn_dict.items():\n",
        "        print(f\"Key: {key}\")\n",
        "        pprint(value)\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator between layers for clarity\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while inspecting hn_dict: {e}\")"
      ],
      "metadata": {
        "id": "Ruz-fk86L0Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can scroll through the output to verify which layers correspond to which end node in your ONNX model. In this dict, each layer is stored under a new name, and it’s original name is a key within the layer under ‘original_names’. You will need this when generating a NMS file for your model, you can find examples NMS configs here."
      ],
      "metadata": {
        "id": "gzARsQW6L9ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Updated NMS layer configuration dictionary\n",
        "nms_layer_config = {\n",
        "    \"nms_scores_th\": 0.3,\n",
        "    \"nms_iou_th\": 0.7,\n",
        "    \"image_dims\": [640, 640],\n",
        "    \"max_proposals_per_class\": 25,\n",
        "    \"classes\": 1,\n",
        "    \"regression_length\": 16,\n",
        "    \"background_removal\": False,\n",
        "    \"background_removal_index\": 0,\n",
        "    \"bbox_decoders\": [\n",
        "        {\n",
        "            \"name\": \"bbox_decoder23\",\n",
        "            \"stride\": 16,\n",
        "            \"reg_layer\": \"conv23\",\n",
        "            \"cls_layer\": \"conv26\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"bbox_decoder38\",\n",
        "            \"stride\": 32,\n",
        "            \"reg_layer\": \"conv38\",\n",
        "            \"cls_layer\": \"conv41\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Path to save the updated JSON configuration\n",
        "output_dir = \"/save/path/\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "output_path = os.path.join(output_dir, \"nms_layer_config.json\")\n",
        "\n",
        "# Save the updated configuration as a JSON file\n",
        "with open(output_path, \"w\") as json_file:\n",
        "    json.dump(nms_layer_config, json_file, indent=4)\n",
        "\n",
        "print(f\"NMS layer configuration saved to {output_path}\")"
      ],
      "metadata": {
        "id": "ggWqQem7MARq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, I made calibration data for the optimization step."
      ],
      "metadata": {
        "id": "zcgJDdLZMEby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Paths to directories and files\n",
        "image_dir = '/input/path'\n",
        "output_dir = '/path/to/output_dir'\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# File paths for saving calibration data\n",
        "calibration_data_path = os.path.join(output_dir, \"calibration_data.npy\")\n",
        "processed_data_path = os.path.join(output_dir, \"processed_calibration_data.npy\")\n",
        "\n",
        "# Initialize an empty list for calibration data\n",
        "calib_data = []\n",
        "\n",
        "# Process all image files in the directory\n",
        "for img_name in os.listdir(image_dir):\n",
        "    img_path = os.path.join(image_dir, img_name)\n",
        "    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img = Image.open(img_path).resize((640, 640))  # Resize to desired dimensions\n",
        "        img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "        calib_data.append(img_array)\n",
        "\n",
        "# Convert the calibration data to a NumPy array\n",
        "calib_data = np.array(calib_data)\n",
        "\n",
        "# Save the normalized calibration data\n",
        "np.save(calibration_data_path, calib_data)\n",
        "print(f\"Normalized calibration dataset saved with shape: {calib_data.shape} to {calibration_data_path}\")\n",
        "\n",
        "# Scale the normalized data back to [0, 255]\n",
        "processed_calibration_data = calib_data * 255.0\n",
        "\n",
        "# Save the processed calibration data\n",
        "np.save(processed_data_path, processed_calibration_data)\n",
        "print(f\"Processed calibration dataset saved with shape: {processed_calibration_data.shape} to {processed_data_path}\")"
      ],
      "metadata": {
        "id": "ifbGGXLdMFsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we’re finally ready to optimize it with this script, you can find sample .alls files here, I referenced yolo10nms.json as a base to create my alls file.\n",
        "\n",
        "Note that the change_output_activation applied to my CLS_layer, you can go back and verify this with Netron like specified above.\n",
        "\n"
      ],
      "metadata": {
        "id": "HpQ99d8JMMG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Define your model's HAR file name\n",
        "model_name = \"modified_run_3_renamed\"\n",
        "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
        "hailo_model_har_name = \"modified_run_3_renamed_hailo_model.har\"\n",
        "\n",
        "# Ensure the HAR file exists\n",
        "assert os.path.isfile(hailo_model_har_name), \"Please provide a valid path for the HAR file\"\n",
        "\n",
        "# Initialize the ClientRunner with the HAR file\n",
        "runner = ClientRunner(har=hailo_model_har_name)\n",
        "\n",
        "# Define the model script to add a normalization layer\n",
        "# Normalization for [0, 1] range\n",
        "alls = \\\"\\\"\\\"\n",
        "normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\n",
        "change_output_activation(conv26, sigmoid)\n",
        "change_output_activation(conv41, sigmoid)\n",
        "nms_postprocess(\"/content/nms_layer_config.json\", meta_arch=yolov8, engine=cpu)\n",
        "performance_param(compiler_optimization_level=max)\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "# Load the model script into the ClientRunner\n",
        "runner.load_model_script(alls)\n",
        "\n",
        "# Define a calibration dataset\n",
        "# Replace 'calib_dataset' with the actual dataset you're using for calibration\n",
        "# For example, if it's a directory of images, prepare the dataset accordingly\n",
        "calib_dataset = \"/content/processed_calibration_data.npy\"\n",
        "\n",
        "# Perform optimization with the calibration dataset\n",
        "runner.optimize(calib_dataset)\n",
        "\n",
        "# Save the optimized model to a new Quantized HAR file\n",
        "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
        "runner.save_har(quantized_model_har_path)\n",
        "\n",
        "print(f\"Quantized HAR file saved to: {quantized_model_har_path}\")"
      ],
      "metadata": {
        "id": "8zXSH2RlMHj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run it"
      ],
      "metadata": {
        "id": "BH33weHVMX08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!my_env/bin/python optimize_model.py"
      ],
      "metadata": {
        "id": "Kfq92L58MayG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling model"
      ],
      "metadata": {
        "id": "Xqvg9ACHMg6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Define the quantized model HAR file\n",
        "model_name = \"modified_run_3_renamed\"\n",
        "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
        "\n",
        "# Initialize the ClientRunner with the HAR file\n",
        "runner = ClientRunner(har=quantized_model_har_path)\n",
        "print(\"[info] ClientRunner initialized successfully.\")\n",
        "\n",
        "# Compile the model\n",
        "try:\n",
        "    hef = runner.compile()\n",
        "    print(\"[info] Compilation completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"[error] Failed to compile the model: {e}\")\n",
        "    raise\n",
        "file_name = f\"{model_name}.hef\"\n",
        "with open(file_name, \"wb\") as f:\n",
        "    f.write(hef)"
      ],
      "metadata": {
        "id": "Ay75npCQMjZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now run"
      ],
      "metadata": {
        "id": "gNppU7pCM94D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!my_env/bin/python compile_model.py\n"
      ],
      "metadata": {
        "id": "bKKPT3ayM-r-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}